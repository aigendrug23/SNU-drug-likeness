{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code in aigenintern/aigenintern1/23-2/MolCLR/230914_MolCLR_Finetune_MTL.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU Status: `gpustat -cuFi 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup args for config\n",
    "from dotmap import DotMap\n",
    "args = DotMap({\n",
    "    'device':'cuda:1',\n",
    "    'learning_config': {\n",
    "        'gin_drop_ratio': 0,    # drop ratio for GIN encoder\n",
    "        'pred_layer_depth': 2,  # dimension of pred layer = feat_dim // 2\n",
    "        'init_pred_lr': 2e-3, # initial learning rate for the prediction head\n",
    "        'init_base_lr': 1e-4, # initial learning rate for the base GIN encoder\n",
    "        'weight_decay': 1e-6,   # weight decay of Adam\n",
    "        'patience': 15, # early stopping patience\n",
    "    },\n",
    "    'scaled' : True, # Solubility, Clearance data will be scaled for training\n",
    "})\n",
    "\n",
    "from src.tdc_constant import TDC\n",
    "# NOTE: data must be in order\n",
    "args.data = TDC.get_ordered_list([\n",
    "    TDC.BBB,\n",
    "#     TDC.Solubility,\n",
    "#     TDC.CYP3A4,\n",
    "#     TDC.Clearance\n",
    "])\n",
    "args.learning_config.num_tasks = len(args.data)\n",
    "\n",
    "args.load_config = DotMap({\n",
    "    'load': False, # If True, do not train. load from ckpts\n",
    "    'name': 'MolCLR_[BBB, CYP3A4, Clearance, Solubility]-12.06_0025', # Will search for ckpts/{name}\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules before run\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MolCLR_[BBB, X, X, X]_sc-12.13_1537',\n",
       " 'ckpts/MolCLR_[BBB, X, X, X]_sc-12.13_1537.pt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_model_name():\n",
    "    from datetime import datetime\n",
    "    li = []\n",
    "    for tdc in TDC.allList:\n",
    "        li.append(str(tdc) if tdc in args.data else 'X')\n",
    "    name = ', '.join(li)\n",
    "    return f'MolCLR_[{name}]{\"_sc\" if args.scaled else \"\"}-{datetime.now().strftime(\"%m.%d_%H%M\")}'\n",
    "\n",
    "\n",
    "model_name= args.load_config.name if args.load_config.load else set_model_name()\n",
    "modelf=f'ckpts/{model_name}.pt'\n",
    "\n",
    "model_name, modelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_config = DotMap({\n",
    "    'batch_size':256, # Don't know well\n",
    "    'num_workers':0,\n",
    "})\n",
    "\n",
    "from src.dataset_mtl import MolTestDatasetWrapper\n",
    "h_dataset = MolTestDatasetWrapper(\n",
    "    tdcList = args.data,\n",
    "    scaled = args.scaled,\n",
    "    batch_size = args.data_config.batch_size,\n",
    "    num_workers = args.data_config.num_workers,\n",
    ")\n",
    "\n",
    "# trainloader: torch_geometric.loader.DataLoader\n",
    "trainloader,validloader,testloader=h_dataset.get_data_loaders()\n",
    "\n",
    "# len = row / batch_size\n",
    "len(trainloader), len(validloader), len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mginet_finetune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GINet_Feat_MTL, load_pre_trained_weights\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGINet_Feat_MTL\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgin_drop_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_layer_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_layer_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_tasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_tasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_act\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m load_pre_trained_weights(model, args\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../aigenintern1/23-2/MolCLR/pretrained_weights/pretrained_gin_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/data/project/aigenintern/aigenintern3/miniconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:673\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    670\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/project/aigenintern/aigenintern3/miniconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:387\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 387\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    391\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    398\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/project/aigenintern/aigenintern3/miniconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:387\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 387\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    391\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    398\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/project/aigenintern/aigenintern3/miniconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:409\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 409\u001b[0m         param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/data/project/aigenintern/aigenintern3/miniconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:671\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    670\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.ginet_finetune import GINet_Feat_MTL, load_pre_trained_weights\n",
    "\n",
    "model = GINet_Feat_MTL(\n",
    "    pool = 'mean',\n",
    "    drop_ratio = args.learning_config.gin_drop_ratio,\n",
    "    pred_layer_depth = args.learning_config.pred_layer_depth,\n",
    "    num_tasks = args.learning_config.num_tasks,\n",
    "    pred_act = 'relu',\n",
    ").to(args.device)\n",
    "model = load_pre_trained_weights(model, args.device, '../aigenintern1/23-2/MolCLR/pretrained_weights/pretrained_gin_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set different learning rates for prediction head and base\n",
    "\n",
    "# 1) check if model_parameters are learnable\n",
    "layer_list = [] # layer_list = prediction head\n",
    "for name, param in model.named_parameters():\n",
    "    if 'pred_head' in name:\n",
    "        print(name, param.requires_grad)\n",
    "        layer_list.append(name)\n",
    "\n",
    "# 2) set different learning rates for prediction head and base\n",
    "# params: prediction head\n",
    "params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in layer_list, model.named_parameters()))))\n",
    "# base_params: base\n",
    "base_params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] not in layer_list, model.named_parameters()))))\n",
    "\n",
    "import torch\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {'params': base_params, 'lr': args.learning_config.init_base_lr},\n",
    "        {'params': params}\n",
    "    ],\n",
    "    args.learning_config.init_pred_lr,\n",
    "    weight_decay = args.learning_config.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "criterions = []\n",
    "for tdc in args.data:\n",
    "    if tdc.isRegression():\n",
    "        criterions.append(nn.MSELoss())\n",
    "    else:\n",
    "        criterions.append(nn.BCEWithLogitsLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mtl_loss(pred, label, criterions):\n",
    "    li = []\n",
    "    # loss_i = criterion(pred[:,i].squeeze(), label[:,i].squeeze())\n",
    "    for i in range(args.learning_config.num_tasks):\n",
    "        label_task = label[:, i].squeeze()\n",
    "        pred_task = pred[:, i].squeeze()\n",
    "        mask = ~torch.isnan(label_task)\n",
    "        # Loss is already divided by len(pred_task[mask]) by nn.MSELoss or nn.BCELoss\n",
    "        # This resolves the problem of potential bias in the loss due to different\n",
    "        # numbers of labels across tasks. The normalization ensures that the loss is\n",
    "        # scale-invariant with respect to the number of elements, making it fair and\n",
    "        # comparable across tasks, even when they have different numbers of labels.\n",
    "        x = criterions[i](pred_task[mask], label_task[mask])\n",
    "        \n",
    "        if not math.isnan(x): # nan if len(pred_task[mask]) == 0\n",
    "            li.append(x)\n",
    "        else:\n",
    "            li.append(torch.tensor(0, device=args.device, dtype=torch.float32))\n",
    "\n",
    "    # loss = mean of each mtl loss & batch\n",
    "    loss = torch.mean(torch.stack(li), dim=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, args, optimizer=optimizer, criterions=criterions):\n",
    "    model.train() # set to train mode\n",
    "    train_loss = 0\n",
    "    for batch in trainloader:\n",
    "        batch = batch.to(args.device)\n",
    "        label = batch.y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch)\n",
    "        \n",
    "        loss = mtl_loss(pred, label, criterions)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(trainloader)\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, loader, args, criterions=criterions):\n",
    "    model.eval()  # Set to eval mode\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(args.device)\n",
    "            label = batch.y\n",
    "            pred = model(batch)\n",
    "\n",
    "            eval_loss += mtl_loss(pred, label, criterions).item()\n",
    "    avg_eval_loss = eval_loss / len(loader)\n",
    "\n",
    "    return avg_eval_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "if args.load_config.load == False:\n",
    "    epoch = 0\n",
    "    print_every_n_epoch = 5\n",
    "\n",
    "    from src.EarlyStopper import EarlyStopper\n",
    "    early_stopper = EarlyStopper(patience=args.learning_config.patience,printfunc=print, \n",
    "                                 verbose=False, path=modelf)\n",
    "    \n",
    "    with open(f'Log: {model_name}.txt', 'a') as fp:\n",
    "        fp.write('Start Training\\n')\n",
    "        fp.write(f'{model_name}\\n')\n",
    "        fp.write(f'{args.device}\\n')\n",
    "        fp.flush()\n",
    "        while True:\n",
    "            epoch+=1\n",
    "            train_loss=train(model,trainloader,args)\n",
    "            valid_loss=eval(model,validloader,args)\n",
    "            fp.write(f'[Epoch{epoch}] train_loss: {train_loss:.4f}, valid_loss: {valid_loss:.4f}. {datetime.now().strftime(\"%H:%M:%S\")}\\n')\n",
    "            fp.flush()\n",
    "            if (epoch % print_every_n_epoch == 0):\n",
    "                pass\n",
    "\n",
    "            early_stopper(valid_loss,model)\n",
    "            if early_stopper.early_stop:\n",
    "                fp.write('early stopping\\n')\n",
    "                fp.flush()\n",
    "                break\n",
    "else:\n",
    "    print('Skip Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(modelf, map_location=args.device))\n",
    "print(f'loaded \"{modelf}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = eval(model,testloader,args)\n",
    "print(f'Final test loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Information\n",
    "print(\"Learning Information\")\n",
    "print(f'Predicting on {args.data}')\n",
    "print(f'Multi-task: predicting {args.learning_config.num_tasks} task')\n",
    "print(f'Criterion: {criterions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test\n",
    "for tdc in args.data:\n",
    "    if not tdc.isRegression():\n",
    "        # Calculate Accuracy\n",
    "        model.eval() # set to eval mode\n",
    "\n",
    "        preds = []\n",
    "        ys = []\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                batch = batch.to(args.device)\n",
    "                label = batch.y\n",
    "                pred = model(batch)\n",
    "                preds.append(pred.reshape(-1))\n",
    "                ys.append(label.reshape(-1))\n",
    "\n",
    "        preds = torch.cat(preds, dim=0) # flatten into 1 dimension\n",
    "        ys = torch.cat(ys, dim=0)\n",
    "\n",
    "        pred_final = nn.Sigmoid()(preds)\n",
    "        correct = (torch.abs(pred_final - ys) < 0.5).float().sum()\n",
    "        accuracy = 100 * correct / len(pred_final)\n",
    "        print(f'Accuracy: {accuracy:.1f}')\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                batch = batch.to(args.device)\n",
    "                pred = model(batch)\n",
    "\n",
    "                print(\"Sample\")\n",
    "                print(batch.smiles[0])\n",
    "                print(f'pred: {pred[0]} label: {batch.y[0]}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sample data\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "for databatch in trainloader:\n",
    "    data = databatch[1]\n",
    "    print(data)\n",
    "    g = to_networkx(data)\n",
    "    nx.draw(g, with_labels=True)\n",
    "    print(data.x[:, 0])\n",
    "    # 0: H, 5: C\n",
    "    print(data.smiles)\n",
    "    # print(g.nodes[0][\"labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
