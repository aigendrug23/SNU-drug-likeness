{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "# gpustat -cuFi 1\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "\n",
    "# learning params\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--hdim', type=float, default=64)\n",
    "parser.add_argument('--batchsize', type=float, default=128)\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'ECFP_MLP_h{args.hdim}b{args.batchsize}_lr{args.lr}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/project/aigenintern/2023-1/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from utils_dm import EarlyStopper, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df = pd.read_csv('../../../2023-2/processed_data/ECFP/BBBP_ECFP_R2B1024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>bbbp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2039 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5  6  7  8  9  ...  1015  1016  1017  1018  1019  1020  \\\n",
       "0     0  1  0  0  1  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "1     0  0  0  0  1  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "2     0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     1     0   \n",
       "3     0  0  1  0  1  0  0  0  0  0  ...     0     0     1     0     0     0   \n",
       "4     0  0  0  0  0  1  0  0  0  0  ...     0     0     0     0     1     0   \n",
       "...  .. .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   ...   \n",
       "2034  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "2035  0  1  0  0  0  0  0  0  0  1  ...     0     0     0     0     1     0   \n",
       "2036  0  1  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "2037  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "2038  0  1  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "\n",
       "      1021  1022  1023  bbbp  \n",
       "0        0     0     0     1  \n",
       "1        0     0     0     1  \n",
       "2        1     0     0     1  \n",
       "3        0     0     0     1  \n",
       "4        0     0     0     1  \n",
       "...    ...   ...   ...   ...  \n",
       "2034     0     0     0     1  \n",
       "2035     0     0     0     1  \n",
       "2036     0     0     0     1  \n",
       "2037     0     0     0     1  \n",
       "2038     0     0     0     1  \n",
       "\n",
       "[2039 rows x 1025 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, labels):\n",
    "        self.dataset = torch.tensor(dataset).float()\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2039,), 1560)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_data = whole_df.values[:, :-1]\n",
    "labels = whole_df.values[:, -1]\n",
    "labels.shape, (labels==1).sum() # check number of positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(whole_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633 203 203\n",
      "1633 203 203\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "test_ratio = 0.1\n",
    "valid_ratio = 0.1\n",
    "\n",
    "test_len = int(len(dataset)*test_ratio)\n",
    "valid_len = int(len(dataset)*valid_ratio)\n",
    "train_len = len(dataset) - valid_len - test_len\n",
    "print(train_len, valid_len, test_len)\n",
    "\n",
    "trainset,validset,testset = torch.utils.data.random_split(dataset, [train_len,valid_len,test_len],\n",
    "                                      torch.Generator().manual_seed(42))\n",
    "print(len(trainset), len(validset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataloader\n",
    "trainloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=False,\n",
    "                        generator=torch.Generator().manual_seed(42))\n",
    "validloader = DataLoader(validset, batch_size=args.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "testloader = DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, hdim, out_dim=1, dropout=0.1):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_dim,hdim),\n",
    "            nn.LayerNorm(hdim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hdim,hdim),\n",
    "            nn.LayerNorm(hdim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hdim,out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = dataset[0][0].shape[0]\n",
    "model = NeuralNetwork(in_dim, args.hdim).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# binary cross entropy nn.BCEWithLogitsLoss()\n",
    "# Mean squared error  nn.MSELoss()\n",
    "\n",
    "early_stopper = EarlyStopper(patience=20,printfunc=print,verbose=True,path=f'ckpts/{model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, args, optimizer=optimizer, criterion=criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, label in trainloader:\n",
    "        batch = batch.to(args.device)\n",
    "        label = label.to(args.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch).squeeze()\n",
    "        \n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, loader, args, return_output=False, criterion=criterion):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch, label in loader:\n",
    "            batch = batch.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            pred = model(batch)\n",
    "            preds.append(pred)\n",
    "            labels.append(label)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    loss = criterion(preds.squeeze(), labels.squeeze())\n",
    "\n",
    "    if return_output:\n",
    "        return loss.item(), preds, labels\n",
    "    else:\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch1] train_loss: 0.6504, valid_loss: 0.5881\n",
      "[Epoch2] train_loss: 0.5482, valid_loss: 0.5402\n",
      "[Epoch3] train_loss: 0.5060, valid_loss: 0.5149\n",
      "[Epoch4] train_loss: 0.4696, valid_loss: 0.4908\n",
      "[Epoch5] train_loss: 0.4444, valid_loss: 0.4663\n",
      "[Epoch6] train_loss: 0.4153, valid_loss: 0.4450\n",
      "[Epoch7] train_loss: 0.3855, valid_loss: 0.4284\n",
      "[Epoch8] train_loss: 0.3641, valid_loss: 0.4124\n",
      "[Epoch9] train_loss: 0.3423, valid_loss: 0.4010\n",
      "[Epoch10] train_loss: 0.3224, valid_loss: 0.3894\n",
      "[Epoch11] train_loss: 0.3067, valid_loss: 0.3803\n",
      "[Epoch12] train_loss: 0.2859, valid_loss: 0.3720\n",
      "[Epoch13] train_loss: 0.2744, valid_loss: 0.3637\n",
      "[Epoch14] train_loss: 0.2552, valid_loss: 0.3573\n",
      "[Epoch15] train_loss: 0.2430, valid_loss: 0.3520\n",
      "[Epoch16] train_loss: 0.2284, valid_loss: 0.3487\n",
      "[Epoch17] train_loss: 0.2133, valid_loss: 0.3423\n",
      "[Epoch18] train_loss: 0.2020, valid_loss: 0.3377\n",
      "[Epoch19] train_loss: 0.1939, valid_loss: 0.3338\n",
      "[Epoch20] train_loss: 0.1829, valid_loss: 0.3328\n",
      "[Epoch21] train_loss: 0.1707, valid_loss: 0.3299\n",
      "[Epoch22] train_loss: 0.1622, valid_loss: 0.3277\n",
      "[Epoch23] train_loss: 0.1549, valid_loss: 0.3257\n",
      "[Epoch24] train_loss: 0.1489, valid_loss: 0.3224\n",
      "[Epoch25] train_loss: 0.1395, valid_loss: 0.3238\n",
      "EarlyStopping counter: 1/20\n",
      "[Epoch26] train_loss: 0.1313, valid_loss: 0.3210\n",
      "[Epoch27] train_loss: 0.1250, valid_loss: 0.3182\n",
      "[Epoch28] train_loss: 0.1227, valid_loss: 0.3180\n",
      "[Epoch29] train_loss: 0.1125, valid_loss: 0.3184\n",
      "EarlyStopping counter: 1/20\n",
      "[Epoch30] train_loss: 0.1084, valid_loss: 0.3176\n",
      "[Epoch31] train_loss: 0.1021, valid_loss: 0.3178\n",
      "EarlyStopping counter: 1/20\n",
      "[Epoch32] train_loss: 0.0972, valid_loss: 0.3210\n",
      "EarlyStopping counter: 2/20\n",
      "[Epoch33] train_loss: 0.0925, valid_loss: 0.3222\n",
      "EarlyStopping counter: 3/20\n",
      "[Epoch34] train_loss: 0.0872, valid_loss: 0.3204\n",
      "EarlyStopping counter: 4/20\n",
      "[Epoch35] train_loss: 0.0868, valid_loss: 0.3216\n",
      "EarlyStopping counter: 5/20\n",
      "[Epoch36] train_loss: 0.0804, valid_loss: 0.3248\n",
      "EarlyStopping counter: 6/20\n",
      "[Epoch37] train_loss: 0.0788, valid_loss: 0.3258\n",
      "EarlyStopping counter: 7/20\n",
      "[Epoch38] train_loss: 0.0736, valid_loss: 0.3302\n",
      "EarlyStopping counter: 8/20\n",
      "[Epoch39] train_loss: 0.0706, valid_loss: 0.3343\n",
      "EarlyStopping counter: 9/20\n",
      "[Epoch40] train_loss: 0.0699, valid_loss: 0.3327\n",
      "EarlyStopping counter: 10/20\n",
      "[Epoch41] train_loss: 0.0671, valid_loss: 0.3366\n",
      "EarlyStopping counter: 11/20\n",
      "[Epoch42] train_loss: 0.0663, valid_loss: 0.3408\n",
      "EarlyStopping counter: 12/20\n",
      "[Epoch43] train_loss: 0.0648, valid_loss: 0.3428\n",
      "EarlyStopping counter: 13/20\n",
      "[Epoch44] train_loss: 0.0593, valid_loss: 0.3458\n",
      "EarlyStopping counter: 14/20\n",
      "[Epoch45] train_loss: 0.0578, valid_loss: 0.3478\n",
      "EarlyStopping counter: 15/20\n",
      "[Epoch46] train_loss: 0.0554, valid_loss: 0.3532\n",
      "EarlyStopping counter: 16/20\n",
      "[Epoch47] train_loss: 0.0531, valid_loss: 0.3552\n",
      "EarlyStopping counter: 17/20\n",
      "[Epoch48] train_loss: 0.0525, valid_loss: 0.3540\n",
      "EarlyStopping counter: 18/20\n",
      "[Epoch49] train_loss: 0.0504, valid_loss: 0.3598\n",
      "EarlyStopping counter: 19/20\n",
      "[Epoch50] train_loss: 0.0472, valid_loss: 0.3671\n",
      "EarlyStopping counter: 20/20\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "while True:\n",
    "    epoch+=1\n",
    "    train_loss = train(model,trainloader,args)\n",
    "    valid_loss = eval(model,validloader,args)\n",
    "    print(f'[Epoch{epoch}] train_loss: {train_loss:.4f}, valid_loss: {valid_loss:.4f}')\n",
    "    early_stopper(valid_loss,model)\n",
    "    if early_stopper.early_stop:\n",
    "        print('early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded best model \"ckpts/ECFP_MLP_h64b32_lr0.0001.pt\", valid loss: 0.3176\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(early_stopper.path, map_location=args.device))\n",
    "model.eval()\n",
    "print(f'loaded best model \"{early_stopper.path}\", valid loss: {early_stopper.val_loss_min:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss: 0.5844\n"
     ]
    }
   ],
   "source": [
    "test_loss = eval(model,testloader,args)**0.5\n",
    "print(f'Final test loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
